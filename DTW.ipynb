{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c90499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dtw import dtw\n",
    "\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "filled_path_df  = pd.read_pickle('data/filled_path_df.pk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cfd70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = filled_path_df.loc['2021-2'].dropna().astype(int)#.tolist()\n",
    "b = filled_path_df.loc['2021-1'].dropna().astype(int)#.tolist()\n",
    "alignment = dtw(a, b, keep_internals=True)\n",
    "normalized_distance = alignment.normalizedDistance\n",
    "print(normalized_distance)\n",
    "## Display the warping curve, i.e. the alignment curve\n",
    "alignment.plot(type=\"threeway\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9504fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for col in filled_path_df.index:\n",
    "    values = filled_path_df.loc[col].dropna().astype(int)    \n",
    "    values = remove_duplicates(values)\n",
    "    X.append(values)\n",
    "X = pd.DataFrame(X, index=resistance_support_table.columns)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8131c433",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.loc[:, X.count(axis=0) > 2]\n",
    "X = X[X.count(axis=1) > 2]\n",
    "max_value = X.max().max()\n",
    "X = X / max_value\n",
    "print(X.shape)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa89611",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Precompute distances\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "def dtw_metric(a, b):\n",
    "    return dtw(a[a > 0], b[b > 0], keep_internals=True).normalizedDistance\n",
    "dtw_matrix = pairwise_distances(X.fillna(0), metric=dtw_metric)\n",
    "print(dtw_matrix.shape)\n",
    "dtw_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d90c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=0.005, min_samples=4, metric='precomputed')\n",
    "labels = dbscan.fit_predict(dtw_matrix)\n",
    "labels = pd.Series(labels)\n",
    "LEN_GRAPH  = N_DAYS * 4 * 21\n",
    "\n",
    "for label in labels.unique():\n",
    "    weeks = X[(labels==label).values].index\n",
    "    cluster_df = total_pathes_df.loc[weeks][:2*LEN_GRAPH]\n",
    "    plt.title(f'Cluster {label} cluster size {cluster_df.shape[0]}')\n",
    "    for diapazon_week in cluster_df.index:\n",
    "        cluster_df.loc[diapazon_week].plot(alpha=0.3, color='grey')\n",
    "    y_major_locator = MultipleLocator(1)  \n",
    "    plt.gca().yaxis.set_major_locator(y_major_locator)\n",
    "    plt.axvline(LEN_GRAPH, color='red', linestyle='--', label='analysis period')\n",
    "    plt.show()\n",
    "    \n",
    "    cluster_X =  -X[(labels==label).values].dropna(how='all', axis=1) * max_value - MIN_VALUE\n",
    "    freq_levels = pd.DataFrame(index=list(range(-8, 8)), dtype=float)\n",
    "    for col in cluster_X.columns:\n",
    "        vc = cluster_X[col].value_counts().sort_index().astype(float)\n",
    "        vc.index = vc.index.astype(int)\n",
    "        freq_levels[col] = vc\n",
    "    freq_levels = freq_levels.fillna(0).sum(axis=1)\n",
    "    freq_levels = freq_levels[freq_levels > 0]\n",
    "    print(freq_levels)\n",
    "    print('-' * 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462f5f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agg_clustering = AgglomerativeClustering(affinity='precomputed', linkage='complete', n_clusters=6)\n",
    "labels = agg_clustering.fit_predict(dtw_matrix)\n",
    "labels = pd.Series(labels)\n",
    "LEN_GRAPH  = N_DAYS * 4 * 21\n",
    "\n",
    "for label in labels.unique():\n",
    "    weeks = X[(labels==label).values].index\n",
    "    cluster_df = total_pathes_df.loc[weeks][:2*LEN_GRAPH]\n",
    "    plt.title(f'Cluster {label} cluster size {cluster_df.shape[0]}')\n",
    "    for diapazon_week in cluster_df.index:\n",
    "        cluster_df.loc[diapazon_week].plot(alpha=0.3, color='grey')\n",
    "    y_major_locator = MultipleLocator(1)  \n",
    "    plt.gca().yaxis.set_major_locator(y_major_locator)\n",
    "    plt.axvline(LEN_GRAPH, color='red', linestyle='--', label='analysis period')\n",
    "    plt.show()\n",
    "    cluster_X =  -X[(labels==label).values].dropna(how='all', axis=1) * max_value - MIN_VALUE\n",
    "    freq_levels = pd.DataFrame(index=list(range(-8, 8)), dtype=float)\n",
    "    for col in cluster_X.columns:\n",
    "        vc = cluster_X[col].value_counts().sort_index().astype(float)\n",
    "        vc.index = vc.index.astype(int)\n",
    "        freq_levels[col] = vc\n",
    "    freq_levels = freq_levels.fillna(0).sum(axis=1)\n",
    "    freq_levels = freq_levels[freq_levels > 0]\n",
    "    print(freq_levels)\n",
    "    print('-' * 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a22330",
   "metadata": {},
   "outputs": [],
   "source": [
    "aff_prop = AffinityPropagation()\n",
    "labels = aff_prop.fit_predict(dtw_matrix)\n",
    "labels = pd.Series(labels)\n",
    "LEN_GRAPH  = N_DAYS * 4 * 21\n",
    "\n",
    "for label in labels.unique():\n",
    "    weeks = X[(labels==label).values].index\n",
    "    cluster_df = total_pathes_df.loc[weeks][:2*LEN_GRAPH]\n",
    "    plt.title(f'Cluster {label} cluster size {cluster_df.shape[0]}')\n",
    "    for diapazon_week in cluster_df.index:\n",
    "        cluster_df.loc[diapazon_week].plot(alpha=0.3, color='grey')\n",
    "    y_major_locator = MultipleLocator(1)  \n",
    "    plt.gca().yaxis.set_major_locator(y_major_locator)\n",
    "    plt.axvline(LEN_GRAPH, color='red', linestyle='--', label='analysis period')\n",
    "    plt.show()\n",
    "    fcluster_X =  -X[(labels==label).values].dropna(how='all', axis=1) * max_value - MIN_VALUE\n",
    "    freq_levels = pd.DataFrame(index=list(range(-8, 8)), dtype=float)\n",
    "    for col in cluster_X.columns:\n",
    "        vc = cluster_X[col].value_counts().sort_index().astype(float)\n",
    "        vc.index = vc.index.astype(int)\n",
    "        freq_levels[col] = vc\n",
    "    freq_levels = freq_levels.fillna(0).sum(axis=1)\n",
    "    freq_levels = freq_levels[freq_levels > 0]\n",
    "    print(freq_levels)\n",
    "    print('-' * 45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a96f45e",
   "metadata": {},
   "source": [
    "# Clustering based on Total Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4829c183",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = total_pathes_df.copy()\n",
    "X = -X.iloc[:, :200] - MIN_VALUE\n",
    "max_value = X.max().max()\n",
    "X = X / max_value\n",
    "X\n",
    "\n",
    "total_pathes_df\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "def dtw_metric(a, b):\n",
    "    return dtw(a[a > 0], b[b > 0], keep_internals=True).normalizedDistance\n",
    "dtw_matrix = pairwise_distances(X.fillna(0), metric=dtw_metric)\n",
    "print(dtw_matrix.shape)\n",
    "dtw_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbf2419",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "agg_clustering = AgglomerativeClustering(affinity='precomputed', linkage='complete', n_clusters=6)\n",
    "labels = agg_clustering.fit_predict(dtw_matrix)\n",
    "labels = pd.Series(labels)\n",
    "for label in labels.unique():\n",
    "    cluster_df = total_pathes_df[(labels==label).values].dropna(how='all', axis=1).copy()\n",
    "    plt.title(f'Cluster {label}. Cluster size: {cluster_df.shape[0]}')\n",
    "    x = pd.DataFrame()\n",
    "    for col in cluster_df.index:\n",
    "        vc = cluster_df.loc[col].value_counts()\n",
    "        x = pd.concat([x, vc], axis=1)\n",
    "    x = x.sum(axis=1) \n",
    "    x.index = -(x.index * max_value) - MIN_VALUE\n",
    "    display(x.sort_values(ascending=False))\n",
    "\n",
    "#     display(cluster_df)\n",
    "    for diapazon_week in cluster_df.index:\n",
    "        # Increase tick frequency on the y-axis\n",
    "        \n",
    "        plot_series = cluster_df.loc[diapazon_week]\n",
    "        \n",
    "        (plot_series).plot(alpha=0.3, color='grey')\n",
    "        y_major_locator = MultipleLocator(1)  # Set the interval between major ticks to 5\n",
    "        plt.gca().yaxis.set_major_locator(y_major_locator)\n",
    "    plt.axvline(200, color='red')\n",
    "    plt.show()\n",
    "\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e116a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=0.005, metric='precomputed')\n",
    "labels = dbscan.fit_predict(dtw_matrix)\n",
    "labels = pd.Series(labels)\n",
    "for label in labels.unique():\n",
    "    cluster_df = total_pathes_df[(labels==label).values].dropna(how='all', axis=1).copy()\n",
    "    plt.title(f'Cluster {label}. Cluster size: {cluster_df.shape[0]}')\n",
    "    x = pd.DataFrame()\n",
    "    for col in cluster_df.index:\n",
    "        vc = cluster_df.loc[col].value_counts()\n",
    "        x = pd.concat([x, vc], axis=1)\n",
    "    x = x.sum(axis=1) \n",
    "    x.index = -(x.index * max_value) - MIN_VALUE\n",
    "    display(x.sort_values(ascending=False))\n",
    "\n",
    "#     display(cluster_df)\n",
    "    for diapazon_week in cluster_df.index:\n",
    "        # Increase tick frequency on the y-axis\n",
    "        \n",
    "        plot_series = cluster_df.loc[diapazon_week]\n",
    "        \n",
    "        (plot_series).plot(alpha=0.3, color='grey')\n",
    "        y_major_locator = MultipleLocator(1)  # Set the interval between major ticks to 5\n",
    "        plt.gca().yaxis.set_major_locator(y_major_locator)\n",
    "    plt.axvline(200, color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de95150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MultipleLocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8710c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = AffinityPropagation(affinity='precomputed')\n",
    "labels = dbscan.fit_predict(dtw_matrix)\n",
    "labels = pd.Series(labels)\n",
    "for label in labels.unique():\n",
    "    cluster_df = total_pathes_df[(labels==label).values].dropna(how='all', axis=1).copy()\n",
    "    plt.title(f'Cluster {label}. Cluster size: {cluster_df.shape[0]}')\n",
    "    x = pd.DataFrame()\n",
    "    for col in cluster_df.index:\n",
    "        vc = cluster_df.loc[col].value_counts()\n",
    "        x = pd.concat([x, vc], axis=1)\n",
    "    x = x.sum(axis=1) \n",
    "    x.index = -(x.index * max_value) - MIN_VALUE\n",
    "    display(x.sort_values(ascending=False))\n",
    "\n",
    "#     display(cluster_df)\n",
    "    for diapazon_week in cluster_df.index:\n",
    "        # Increase tick frequency on the y-axis\n",
    "        \n",
    "        plot_series = cluster_df.loc[diapazon_week]\n",
    "        \n",
    "        (plot_series).plot(alpha=0.3, color='grey')\n",
    "        y_major_locator = MultipleLocator(1)  # Set the interval between major ticks to 5\n",
    "        plt.gca().yaxis.set_major_locator(y_major_locator)\n",
    "    plt.axvline(200, color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32d3dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52a4018",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_WEEKS = 2\n",
    "N_DAYS = 2 * 5\n",
    "TRAIN_SIZE = int(es_df.groupby('calendar_week')['close'].count().mean() * N_WEEKS)\n",
    "target_size = TRAIN_SIZE // 2\n",
    "train_X = total_pathes_df.iloc[:, :TRAIN_SIZE]\n",
    "target_pathes_df = total_pathes_df.iloc[:, TRAIN_SIZE: TRAIN_SIZE+target_size]\n",
    "is_null_all = target_pathes_df.isnull().all(axis=1)\n",
    "paths_X = train_X[~is_null_all]\n",
    "paths_X.columns = 'path_' + paths_X.columns.astype(str)\n",
    "min_harmonic_target = target_pathes_df.min(axis=1)[~is_null_all].astype(int)\n",
    "max_harmonic_target = target_pathes_df.max(axis=1)[~is_null_all].astype(int)\n",
    "min_harmonic_target.shape, max_harmonic_target.shape, paths_X.shape\n",
    "\n",
    "filled_path_df = create_filled_path_df(resistance_support_table, N_DAYS)\n",
    "X_paths = filled_path_df.copy()\n",
    "X_paths.shape\n",
    "\n",
    "value2streak_length_dict = {}\n",
    "for col in resistance_support_table.columns:\n",
    "    values = resistance_support_table[col].dropna()#.loc[:'2020-']\n",
    "    min_date = diapazon_week2harmon_start_date[col]\n",
    "    max_date = min_date + pd.Timedelta(N_DAYS, 'days')\n",
    "    values = values[min_date:max_date]\n",
    "#     values = pd.Series(remove_duplicates(values))\n",
    "    value2streak_length = defaultdict(int)\n",
    "    \n",
    "    # find number of stuff\n",
    "    harmonic2last_meet_len = defaultdict(int)\n",
    "    for query_harmonic in range(-7, 8):\n",
    "        for resistance_support_value in values:\n",
    "            if int(resistance_support_value[:-1]) == query_harmonic:\n",
    "                value2streak_length[query_harmonic] += 1\n",
    "                harmonic2last_meet_len[query_harmonic] = 0\n",
    "            harmonic2last_meet_len[query_harmonic] += 1\n",
    "            \n",
    "            if harmonic2last_meet_len[query_harmonic] > 2:\n",
    "                break\n",
    "    value2streak_length_dict[col] = value2streak_length\n",
    "    \n",
    "\n",
    "X_res_sup = pd.DataFrame(value2streak_length_dict).T \n",
    "max_value = X_res_sup.max().max()\n",
    "print(max_value)\n",
    "X_res_sup = X_res_sup / max_value\n",
    "X_res_sup = X_res_sup.sort_index(axis=1)\n",
    "X_res_sup = X_res_sup.loc[:, X_res_sup.count() > 2].fillna(0)\n",
    "X_res_sup.columns = X_res_sup.columns.astype(str) + '_freq'\n",
    "X_paths = pd.concat([X_paths, X_res_sup, paths_X], axis=1)\n",
    "X_paths = X_paths[~is_null_all]\n",
    "X_paths['week_number'] = X_paths.index.map(diapazan_week2weeknumber)\n",
    "X_paths['diapazon_width'] = X_paths.index.map(diapazon_ser)\n",
    "X_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beebc3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import LeaveOneOut, cross_val_predict\n",
    "cv_pred = cross_val_predict(LGBMClassifier(random_state=123, boosting_type='dart'), \n",
    "                            X_paths, max_harmonic_target, cv=LeaveOneOut())\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(cv_pred, max_harmonic_target))\n",
    "\n",
    "model = LGBMClassifier(random_state=123, boosting_type='dart')\n",
    "model.fit(X_paths, max_harmonic_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e838017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "min_len = min(array1.shape[0], array2.shape[0])\n",
    "jaccard_similarity = jaccard_score(array1[:min_len].values, \n",
    "                                   array2[:min_len].values, average='macro')\n",
    "jaccard_similarity\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "jaccard_similarity_dict = {}\n",
    "for col1 in tqdm(resistance_support_table.columns):\n",
    "    jaccard_similarity_dict[col1] = {}\n",
    "    for col2 in resistance_support_table.drop(col1, axis=1).columns:\n",
    "        array1 = resistance_support_table[col1].dropna()\n",
    "        array2 = resistance_support_table[col2].dropna()\n",
    "        min_len = min(array1.shape[0], array2.shape[0])\n",
    "        jaccard_similarity = jaccard_score(array1[:min_len], array2[:min_len], average='macro')\n",
    "        jaccard_similarity_dict[col1][col2] = jaccard_similarity\n",
    "\n",
    "pd.DataFrame(jaccard_similarity_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
